{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab5deac",
   "metadata": {
    "cellUniqueIdByVincent": "d9a61"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import logging\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5d4b251",
   "metadata": {
    "cellUniqueIdByVincent": "0210d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d4bbab",
   "metadata": {
    "cellUniqueIdByVincent": "1fc1d"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to extract sentences from PDFs using PyPDF2\n",
    "def read_pdf_sentences(file_path):\n",
    "    sentences = []\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    sentences.extend(sent_tokenize(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {file_path}: {str(e)}\")\n",
    "    return sentences\n",
    "\n",
    "def extract_and_merge(pdf_path, csv_path):\n",
    "    try:\n",
    "        print(f\"Attempting to read PDF files from: {pdf_path}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF directory not found: {pdf_path}\")\n",
    "        \n",
    "        pdf_files = [os.path.join(pdf_path, file) for file in os.listdir(pdf_path) if file.endswith('.pdf')]\n",
    "        print(f\"Found {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        pdf_sentences = []\n",
    "        for file in pdf_files:\n",
    "            pdf_sentences.extend(read_pdf_sentences(file))\n",
    "        \n",
    "        print(f\"Extracted {len(pdf_sentences)} sentences from PDF files\")\n",
    "        pdf_df = pd.DataFrame({'content': pdf_sentences})\n",
    "        \n",
    "        print(f\"Attempting to read CSV file: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "        \n",
    "        news_data = pd.read_csv(csv_path, encoding='latin1')\n",
    "        content_column = next((col for col in news_data.columns if col.lower().strip() == 'content'), None)\n",
    "        if content_column is None:\n",
    "            raise KeyError(f\"No 'content' column found in the CSV file: {csv_path}\")\n",
    "    \n",
    "        news_data_paragraphs = []\n",
    "        for content in news_data[content_column].dropna():\n",
    "            paragraphs = content.split('\\n\\n')\n",
    "            news_data_paragraphs.extend(paragraphs)\n",
    "        \n",
    "        print(f\"Extracted {len(news_data_paragraphs)} paragraphs from CSV file\")\n",
    "        news_df = pd.DataFrame({'content': news_data_paragraphs})\n",
    "        \n",
    "        merged_data = pd.concat([pdf_df, news_df], ignore_index=True)\n",
    "        print(f\"Merged data shape: {merged_data.shape}\")\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_and_merge: {str(e)}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Contents of current directory: {os.listdir('.')}\")\n",
    "        if os.path.exists(pdf_path):\n",
    "            print(f\"Contents of PDF directory: {os.listdir(pdf_path)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6351c90",
   "metadata": {
    "cellUniqueIdByVincent": "9d7b5"
   },
   "outputs": [],
   "source": [
    "# Assign Sentiment Analyzer Score\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def assign_sentiment_scores(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "def assign_scores(data):\n",
    "    data['sentiment'] = data['content'].apply(assign_sentiment_scores)\n",
    "    return data\n",
    "\n",
    "# Function to assign direction and new_direction based on sentiment scores\n",
    "def assign_directions(data):\n",
    "    data['direction'] = data['sentiment'].apply(lambda x: 'bearish' if x < 0.0 else ('neutral' if 0.0 <= x < 0.4 else 'bullish'))\n",
    "    data['new_direction'] = data['sentiment'].apply(lambda x: 2 if x < 0.0 else (1 if 0.0 <= x < 0.4 else 0))\n",
    "    return data\n",
    "\n",
    "# Function to preprocess individual text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, '')\n",
    "\n",
    "    # Remove emoticons (this is a basic implementation, might need refinement)\n",
    "    text = re.sub(r'[:;=]-?[()DPp]', '', text)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    except LookupError:\n",
    "        # If lemmatization fails, just use the original tokens\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to preprocess the entire DataFrame\n",
    "def preprocess_data(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['content'] = df_cleaned['content'].apply(preprocess_text)\n",
    "    return df_cleaned\n",
    "\n",
    "# Count the number of bearish, bullish, and neutral sentiments\n",
    "def sentiment_counts(data):\n",
    "    return data['direction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21df04a9",
   "metadata": {
    "cellUniqueIdByVincent": "5fa23"
   },
   "outputs": [],
   "source": [
    "# Prepare Dataset Function\n",
    "def prepare_dataset(data, sample_frac=0.1, random_state=42):\n",
    "    print(\"Preparing dataset...\")\n",
    "    data = data.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    X = data['content']\n",
    "    y = data['new_direction']\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Define resampling strategy\n",
    "    over = SMOTE(sampling_strategy='auto', random_state=random_state)\n",
    "    under = RandomUnderSampler(sampling_strategy='auto', random_state=random_state)\n",
    "    \n",
    "    # Create a pipeline with SMOTE and RandomUnderSampler\n",
    "    resampling = Pipeline([('over', over), ('under', under)])\n",
    "    \n",
    "    # Apply resampling\n",
    "    X_train_resampled, y_train_resampled = resampling.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Dataset prepared with train size: {X_train_resampled.shape[0]} and test size: {X_test.shape[0]}\")\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff447d57",
   "metadata": {
    "cellUniqueIdByVincent": "5a0f4"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training and evaluating model...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize AdaBoost classifier\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=3)\n",
    "        \n",
    "        # Check scikit-learn version and use appropriate parameter\n",
    "        if sklearn.__version__ >= '0.22':\n",
    "            model = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "        else:\n",
    "            model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y_test, y_pred, target_names=['bullish', 'neutral', 'bearish'], output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        return model, {'accuracy': accuracy}, report_df, y_test, y_pred, cm\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in train_and_evaluate: {str(e)}\")\n",
    "        print(f\"Error details: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d1d0ab",
   "metadata": {
    "cellUniqueIdByVincent": "b95e2"
   },
   "outputs": [],
   "source": [
    "def create_comprehensive_report(company_name, metrics, report_df, cm):\n",
    "    # Create confusion matrix DataFrame\n",
    "    cm_df = pd.DataFrame(cm, index=['True Bullish', 'True Neutral', 'True Bearish'],\n",
    "                         columns=['Pred Bullish', 'Pred Neutral', 'Pred Bearish'])\n",
    "    \n",
    "    # Prepare data for the comprehensive report\n",
    "    report_data = {\n",
    "        'Company': company_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Confusion Matrix': cm_df.to_json(),\n",
    "    }\n",
    "    \n",
    "    # Add precision, recall, and F1-score for each class\n",
    "    for class_label, class_name in zip(['bullish', 'neutral', 'bearish'], ['Bullish', 'Neutral', 'Bearish']):\n",
    "        if class_label in report_df.index:\n",
    "            report_data.update({\n",
    "                f'Precision ({class_name})': report_df.loc[class_label, 'precision'],\n",
    "                f'Recall ({class_name})': report_df.loc[class_label, 'recall'],\n",
    "                f'F1-Score ({class_name})': report_df.loc[class_label, 'f1-score'],\n",
    "            })\n",
    "        else:\n",
    "            report_data.update({\n",
    "                f'Precision ({class_name})': None,\n",
    "                f'Recall ({class_name})': None,\n",
    "                f'F1-Score ({class_name})': None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame([report_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bf3180a",
   "metadata": {
    "cellUniqueIdByVincent": "5dcfa"
   },
   "outputs": [],
   "source": [
    "def main(company_name, pdf_path, csv_path):\n",
    "    try:\n",
    "        logger.info(f\"Processing {company_name}...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        raw_data = extract_and_merge(pdf_path, csv_path)\n",
    "        data_with_sentiment = assign_scores(raw_data)\n",
    "        data_with_directions = assign_directions(data_with_sentiment)\n",
    "        cleaned_data = preprocess_data(data_with_directions)\n",
    "\n",
    "        # Display sentiment counts\n",
    "        counts = sentiment_counts(cleaned_data)\n",
    "        logger.info(f\"{company_name} Sentiment Counts:\")\n",
    "        logger.info(counts)\n",
    "\n",
    "        # Prepare dataset\n",
    "        X_train, X_test, y_train, y_test, vectorizer = prepare_dataset(cleaned_data)\n",
    "\n",
    "        # Train and evaluate\n",
    "        result = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "        if result is None:\n",
    "            logger.error(f\"Training and evaluation failed for {company_name}\")\n",
    "            return None\n",
    "\n",
    "        model, metrics, report_df, y_test, y_pred, cm = result\n",
    "\n",
    "        # Display the evaluation metrics\n",
    "        logger.info(f\"Evaluation Metrics for {company_name}:\")\n",
    "        logger.info(f\"Accuracy: {metrics['accuracy']}\")\n",
    "    \n",
    "        # Display the classification report\n",
    "        logger.info(f\"Classification Report for {company_name}:\")\n",
    "        logger.info(report_df)\n",
    "\n",
    "        # Create comprehensive report\n",
    "        comprehensive_report = create_comprehensive_report(company_name, metrics, report_df, cm)\n",
    "\n",
    "        return comprehensive_report\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {company_name}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bada2b75",
   "metadata": {
    "cellUniqueIdByVincent": "032f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing for Lloyds\n",
      "INFO:__main__:Processing Lloyds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read PDF files from: data/lloyds\n",
      "Found 20 PDF files\n",
      "Extracted 66875 sentences from PDF files\n",
      "Attempting to read CSV file: data/lloyds/lloyds_news.csv\n",
      "Extracted 1834 paragraphs from CSV file\n",
      "Merged data shape: (68709, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Lloyds Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    27490\n",
      "bullish    26909\n",
      "bearish    14310\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 6618 and test size: 1375\n",
      "Training and evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Evaluation Metrics for Lloyds:\n",
      "INFO:__main__:Accuracy: 0.7258181818181818\n",
      "INFO:__main__:Classification Report for Lloyds:\n",
      "INFO:__main__:              precision    recall  f1-score      support\n",
      "bullish        0.817797  0.706960  0.758350   546.000000\n",
      "neutral        0.658940  0.750943  0.701940   530.000000\n",
      "bearish        0.715719  0.715719  0.715719   299.000000\n",
      "accuracy       0.725818  0.725818  0.725818     0.725818\n",
      "macro avg      0.730819  0.724541  0.725336  1375.000000\n",
      "weighted avg   0.734368  0.725818  0.727336  1375.000000\n",
      "INFO:__main__:Starting processing for IAG\n",
      "INFO:__main__:Processing IAG...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read PDF files from: data/iag\n",
      "Found 11 PDF files\n",
      "Extracted 34291 sentences from PDF files\n",
      "Attempting to read CSV file: data/iag/iag_news.csv\n",
      "Extracted 2037 paragraphs from CSV file\n",
      "Merged data shape: (36328, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:IAG Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    17607\n",
      "bullish    12229\n",
      "bearish     6492\n",
      "Name: count, dtype: int64\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 4182 and test size: 727\n",
      "Training and evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation Metrics for IAG:\n",
      "INFO:__main__:Accuracy: 0.7042640990371389\n",
      "INFO:__main__:Classification Report for IAG:\n",
      "INFO:__main__:              precision    recall  f1-score     support\n",
      "bullish        0.740196  0.592157  0.657952  255.000000\n",
      "neutral        0.687943  0.841040  0.756827  346.000000\n",
      "bearish        0.700000  0.555556  0.619469  126.000000\n",
      "accuracy       0.704264  0.704264  0.704264    0.704264\n",
      "macro avg      0.709380  0.662918  0.678083  727.000000\n",
      "weighted avg   0.708361  0.704264  0.698340  727.000000\n",
      "INFO:__main__:Starting processing for Vodafone\n",
      "INFO:__main__:Processing Vodafone...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read PDF files from: data/vodafone\n",
      "Found 14 PDF files\n",
      "Extracted 51164 sentences from PDF files\n",
      "Attempting to read CSV file: data/vodafone/vodafone_news.csv\n",
      "Extracted 0 paragraphs from CSV file\n",
      "Merged data shape: (51164, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vodafone Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    24998\n",
      "bullish    18868\n",
      "bearish     7298\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 5721 and test size: 1024\n",
      "Training and evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Evaluation Metrics for Vodafone:\n",
      "INFO:__main__:Accuracy: 0.7734375\n",
      "INFO:__main__:Classification Report for Vodafone:\n",
      "INFO:__main__:              precision    recall  f1-score      support\n",
      "bullish        0.811209  0.747283  0.777935   368.000000\n",
      "neutral        0.781885  0.826172  0.803419   512.000000\n",
      "bearish        0.652778  0.652778  0.652778   144.000000\n",
      "accuracy       0.773438  0.773438  0.773438     0.773438\n",
      "macro avg      0.748624  0.742077  0.744711  1024.000000\n",
      "weighted avg   0.774268  0.773438  0.773077  1024.000000\n",
      "INFO:__main__:Comprehensive classification report for all companies saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define paths for each company\n",
    "    companies = {\n",
    "        'Lloyds': {\n",
    "            'pdf_path': 'data/lloyds',\n",
    "            'csv_path': 'data/lloyds/lloyds_news.csv'\n",
    "        },\n",
    "        'IAG': {\n",
    "            'pdf_path': 'data/iag',\n",
    "            'csv_path': 'data/iag/iag_news.csv'\n",
    "        },\n",
    "        'Vodafone': {\n",
    "            'pdf_path': 'data/vodafone',\n",
    "            'csv_path': 'data/vodafone/vodafone_news.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    all_reports = []\n",
    "\n",
    "    for company_name, paths in companies.items():\n",
    "        try:\n",
    "            logger.info(f\"Starting processing for {company_name}\")\n",
    "            company_report = main(company_name, paths['pdf_path'], paths['csv_path'])\n",
    "            \n",
    "            if company_report is not None:\n",
    "                all_reports.append(company_report)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {company_name}: {str(e)}\")\n",
    "\n",
    "    # Combine all reports into a single DataFrame\n",
    "    if all_reports:\n",
    "        combined_report = pd.concat(all_reports, ignore_index=True)\n",
    "        combined_report.to_csv('comprehensive_classification_report_adaboost.csv', index=False)\n",
    "        logger.info(\"Comprehensive classification report for all companies saved to CSV.\")\n",
    "    else:\n",
    "        logger.warning(\"No reports were generated.\")"
   ]
  }
 ],
 "metadata": {
  "vincent": {
   "sessionId": "38ba926bc62326555be521cd_2025-05-29T11-08-59-559Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
