{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab5deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d4b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d4bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to extract sentences from PDFs using PyPDF2\n",
    "def read_pdf_sentences(file_path):\n",
    "    sentences = []\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    sentences.extend(sent_tokenize(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {file_path}: {str(e)}\")\n",
    "    return sentences\n",
    "\n",
    "def extract_and_merge(pdf_path, csv_path):\n",
    "    try:\n",
    "        print(f\"Attempting to read PDF files from: {pdf_path}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF directory not found: {pdf_path}\")\n",
    "        \n",
    "        pdf_files = [os.path.join(pdf_path, file) for file in os.listdir(pdf_path) if file.endswith('.pdf')]\n",
    "        print(f\"Found {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        pdf_sentences = []\n",
    "        for file in pdf_files:\n",
    "            pdf_sentences.extend(read_pdf_sentences(file))\n",
    "        \n",
    "        print(f\"Extracted {len(pdf_sentences)} sentences from PDF files\")\n",
    "        pdf_df = pd.DataFrame({'content': pdf_sentences})\n",
    "        \n",
    "        print(f\"Attempting to read CSV file: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "        \n",
    "        news_data = pd.read_csv(csv_path, encoding='latin1')\n",
    "        content_column = next((col for col in news_data.columns if col.lower().strip() == 'content'), None)\n",
    "        if content_column is None:\n",
    "            raise KeyError(f\"No 'content' column found in the CSV file: {csv_path}\")\n",
    "    \n",
    "        news_data_paragraphs = []\n",
    "        for content in news_data[content_column].dropna():\n",
    "            paragraphs = content.split('\\n\\n')\n",
    "            news_data_paragraphs.extend(paragraphs)\n",
    "        \n",
    "        print(f\"Extracted {len(news_data_paragraphs)} paragraphs from CSV file\")\n",
    "        news_df = pd.DataFrame({'content': news_data_paragraphs})\n",
    "        \n",
    "        merged_data = pd.concat([pdf_df, news_df], ignore_index=True)\n",
    "        print(f\"Merged data shape: {merged_data.shape}\")\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_and_merge: {str(e)}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Contents of current directory: {os.listdir('.')}\")\n",
    "        if os.path.exists(pdf_path):\n",
    "            print(f\"Contents of PDF directory: {os.listdir(pdf_path)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6351c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Sentiment Analyzer Score\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def assign_sentiment_scores(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "def assign_scores(data):\n",
    "    data['sentiment'] = data['content'].apply(assign_sentiment_scores)\n",
    "    return data\n",
    "\n",
    "# Function to assign direction and new_direction based on sentiment scores\n",
    "def assign_directions(data):\n",
    "    data['direction'] = data['sentiment'].apply(lambda x: 'bearish' if x < 0.0 else ('neutral' if 0.0 <= x < 0.4 else 'bullish'))\n",
    "    data['new_direction'] = data['sentiment'].apply(lambda x: 2 if x < 0.0 else (1 if 0.0 <= x < 0.4 else 0))\n",
    "    return data\n",
    "\n",
    "# Function to preprocess individual text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, '')\n",
    "\n",
    "    # Remove emoticons (this is a basic implementation, might need refinement)\n",
    "    text = re.sub(r'[:;=]-?[()DPp]', '', text)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    except LookupError:\n",
    "        # If lemmatization fails, just use the original tokens\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to preprocess the entire DataFrame\n",
    "def preprocess_data(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['content'] = df_cleaned['content'].apply(preprocess_text)\n",
    "    return df_cleaned\n",
    "\n",
    "# Count the number of bearish, bullish, and neutral sentiments\n",
    "def sentiment_counts(data):\n",
    "    return data['direction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21df04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset Function\n",
    "def prepare_dataset(data, sample_frac=0.1, random_state=42):\n",
    "    print(\"Preparing dataset...\")\n",
    "    data = data.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    X = data['content']\n",
    "    y = data['new_direction']\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Define resampling strategy\n",
    "    over = SMOTE(sampling_strategy='auto', random_state=random_state)\n",
    "    under = RandomUnderSampler(sampling_strategy='auto', random_state=random_state)\n",
    "    \n",
    "    # Create a pipeline with SMOTE and RandomUnderSampler\n",
    "    resampling = Pipeline([('over', over), ('under', under)])\n",
    "    \n",
    "    # Apply resampling\n",
    "    X_train_resampled, y_train_resampled = resampling.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Dataset prepared with train size: {X_train_resampled.shape[0]} and test size: {X_test.shape[0]}\")\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff447d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def train_and_evaluate_multiple_models(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training and evaluating multiple models...\")\n",
    "    \n",
    "    models = {\n",
    "        'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=50, random_state=42),\n",
    "        'SVM': SVC(kernel='rbf', random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42),\n",
    "        'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        for name, model in models.items():\n",
    "            print(f\"Training and evaluating {name}...\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Generate classification report\n",
    "            report = classification_report(y_test, y_pred, target_names=['bullish', 'neutral', 'bearish'], output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            \n",
    "            # Generate confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'accuracy': accuracy,\n",
    "                'report': report_df,\n",
    "                'confusion_matrix': cm,\n",
    "                'cv_scores': cv_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - Accuracy: {accuracy}, Cross-validation mean score: {np.mean(cv_scores)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in train_and_evaluate_multiple_models: {str(e)}\")\n",
    "        print(f\"Error details: {traceback.format_exc()}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d1d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_report_multiple_models(company_name, results):\n",
    "    report_data = {'Company': company_name}\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        report_data[f'{model_name} Accuracy'] = model_results['accuracy']\n",
    "        report_data[f'{model_name} CV Mean Score'] = np.mean(model_results['cv_scores'])\n",
    "        report_data[f'{model_name} CV Std Score'] = np.std(model_results['cv_scores'])\n",
    "        report_data[f'{model_name} Confusion Matrix'] = model_results['confusion_matrix'].tolist()\n",
    "        \n",
    "        for class_name in ['bullish', 'neutral', 'bearish']:\n",
    "            if class_name in model_results['report'].index:\n",
    "                report_data[f'{model_name} Precision ({class_name})'] = model_results['report'].loc[class_name, 'precision']\n",
    "                report_data[f'{model_name} Recall ({class_name})'] = model_results['report'].loc[class_name, 'recall']\n",
    "                report_data[f'{model_name} F1-Score ({class_name})'] = model_results['report'].loc[class_name, 'f1-score']\n",
    "    \n",
    "    return pd.DataFrame([report_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf3180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(company_name, pdf_path, csv_path):\n",
    "    try:\n",
    "        logger.info(f\"Processing {company_name}...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        raw_data = extract_and_merge(pdf_path, csv_path)\n",
    "        data_with_sentiment = assign_scores(raw_data)\n",
    "        data_with_directions = assign_directions(data_with_sentiment)\n",
    "        cleaned_data = preprocess_data(data_with_directions)\n",
    "\n",
    "        # Display sentiment counts\n",
    "        counts = sentiment_counts(cleaned_data)\n",
    "        logger.info(f\"{company_name} Sentiment Counts:\")\n",
    "        logger.info(counts)\n",
    "\n",
    "        # Prepare dataset\n",
    "        X_train, X_test, y_train, y_test, vectorizer = prepare_dataset(cleaned_data)\n",
    "\n",
    "        # Train and evaluate multiple models\n",
    "        results = train_and_evaluate_multiple_models(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "        if results is None:\n",
    "            logger.error(f\"Training and evaluation failed for {company_name}\")\n",
    "            return None\n",
    "\n",
    "        # Create comprehensive report\n",
    "        comprehensive_report = create_comprehensive_report_multiple_models(company_name, results)\n",
    "\n",
    "        return comprehensive_report\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {company_name}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bada2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing for Lloyds\n",
      "INFO:__main__:Processing Lloyds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read PDF files from: data/lloyds\n",
      "Found 20 PDF files\n",
      "Extracted 66875 sentences from PDF files\n",
      "Attempting to read CSV file: data/lloyds/lloyds_news.csv\n",
      "Extracted 1834 paragraphs from CSV file\n",
      "Merged data shape: (68709, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Lloyds Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    27490\n",
      "bullish    26909\n",
      "bearish    14310\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 6618 and test size: 1375\n",
      "Training and evaluating multiple models...\n",
      "Training and evaluating AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost - Accuracy: 0.7258181818181818, Cross-validation mean score: 0.7582316578863839\n",
      "Training and evaluating SVM...\n",
      "SVM - Accuracy: 0.7934545454545454, Cross-validation mean score: 0.8396774016756753\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest - Accuracy: 0.7978181818181819, Cross-validation mean score: 0.8451191218347024\n",
      "Training and evaluating Naive Bayes...\n",
      "Naive Bayes - Accuracy: 0.6567272727272727, Cross-validation mean score: 0.7064107482536486\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7745454545454545, Cross-validation mean score: 0.8117239040631358\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing for IAG\n",
      "INFO:__main__:Processing IAG...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - Accuracy: 0.7629090909090909, Cross-validation mean score: 0.8188256571510779\n",
      "Attempting to read PDF files from: data/iag\n",
      "Found 11 PDF files\n",
      "Extracted 34291 sentences from PDF files\n",
      "Attempting to read CSV file: data/iag/iag_news.csv\n",
      "Extracted 2037 paragraphs from CSV file\n",
      "Merged data shape: (36328, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:IAG Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    17607\n",
      "bullish    12229\n",
      "bearish     6492\n",
      "Name: count, dtype: int64\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 4182 and test size: 727\n",
      "Training and evaluating multiple models...\n",
      "Training and evaluating AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost - Accuracy: 0.7042640990371389, Cross-validation mean score: 0.7400842036665466\n",
      "Training and evaluating SVM...\n",
      "SVM - Accuracy: 0.7372764786795049, Cross-validation mean score: 0.8804462279844284\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest - Accuracy: 0.781292984869326, Cross-validation mean score: 0.8639521988418423\n",
      "Training and evaluating Naive Bayes...\n",
      "Naive Bayes - Accuracy: 0.657496561210454, Cross-validation mean score: 0.7613652083940708\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7359009628610729, Cross-validation mean score: 0.8388385839149846\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing for Vodafone\n",
      "INFO:__main__:Processing Vodafone...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - Accuracy: 0.71939477303989, Cross-validation mean score: 0.8570095407956189\n",
      "Attempting to read PDF files from: data/vodafone\n",
      "Found 14 PDF files\n",
      "Extracted 51164 sentences from PDF files\n",
      "Attempting to read CSV file: data/vodafone/vodafone_news.csv\n",
      "Extracted 0 paragraphs from CSV file\n",
      "Merged data shape: (51164, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vodafone Sentiment Counts:\n",
      "INFO:__main__:direction\n",
      "neutral    24998\n",
      "bullish    18868\n",
      "bearish     7298\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with train size: 5721 and test size: 1024\n",
      "Training and evaluating multiple models...\n",
      "Training and evaluating AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ELITEBOOK\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost - Accuracy: 0.7734375, Cross-validation mean score: 0.7722460072678413\n",
      "Training and evaluating SVM...\n",
      "SVM - Accuracy: 0.802734375, Cross-validation mean score: 0.8739760894127707\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest - Accuracy: 0.8359375, Cross-validation mean score: 0.8783462607261734\n",
      "Training and evaluating Naive Bayes...\n",
      "Naive Bayes - Accuracy: 0.6748046875, Cross-validation mean score: 0.7545897334106941\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7861328125, Cross-validation mean score: 0.8362184322227991\n",
      "Training and evaluating Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Comprehensive classification report for all companies saved to CSV.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - Accuracy: 0.7822265625, Cross-validation mean score: 0.8680312700400036\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define paths for each company\n",
    "    companies = {\n",
    "        'Lloyds': {\n",
    "            'pdf_path': 'data/lloyds',\n",
    "            'csv_path': 'data/lloyds/lloyds_news.csv'\n",
    "        },\n",
    "        'IAG': {\n",
    "            'pdf_path': 'data/iag',\n",
    "            'csv_path': 'data/iag/iag_news.csv'\n",
    "        },\n",
    "        'Vodafone': {\n",
    "            'pdf_path': 'data/vodafone',\n",
    "            'csv_path': 'data/vodafone/vodafone_news.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    all_reports = []\n",
    "\n",
    "    for company_name, paths in companies.items():\n",
    "        try:\n",
    "            logger.info(f\"Starting processing for {company_name}\")\n",
    "            company_report = main(company_name, paths['pdf_path'], paths['csv_path'])\n",
    "            \n",
    "            if company_report is not None:\n",
    "                all_reports.append(company_report)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {company_name}: {str(e)}\")\n",
    "\n",
    "    # Combine all reports into a single DataFrame\n",
    "    if all_reports:\n",
    "        combined_report = pd.concat(all_reports, ignore_index=True)\n",
    "        combined_report.to_csv('comprehensive_classification_report_adaboost.csv', index=False)\n",
    "        logger.info(\"Comprehensive classification report for all companies saved to CSV.\")\n",
    "    else:\n",
    "        logger.warning(\"No reports were generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
